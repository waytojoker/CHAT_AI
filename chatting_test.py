import time
import ollama
import streamlit as st
import re
maxHistoryMessages = 10

# åˆå§‹åŒ–å¯¹è¯å†å²
if "message" not in st.session_state:
    st.session_state["message"] = []

# Ollama å®¢æˆ·ç«¯
client = ollama.Client(host="http://127.0.0.1:11434")

# åˆå§‹åŒ–é…ç½®å‚æ•°
if "role_config" not in st.session_state:
    st.session_state['role_config'] = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜å¹¶æä¾›å¸®åŠ©ã€‚"

if "scene_config" not in st.session_state:
    st.session_state['scene_config'] = "åœ¨ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„å¯¹è¯ç¯å¢ƒä¸­"

if "task_config" not in st.session_state:
    st.session_state['task_config'] = "è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”"

if "temperature" not in st.session_state:
    st.session_state['temperature'] = 0.7

def preprocess_output(output):
    # æ›¿æ¢ $$...$$ åŒ…è£¹çš„å…¬å¼ä¸º st.latex å¯è¯†åˆ«çš„å½¢å¼
    # ä¾‹å¦‚ï¼š$$\boxed{8}$$ â†’ \boxed{8}
    output = output.replace("<think>", "\n\n**æ€è€ƒï¼š**\n")
    output = output.replace("</think>", "\n\n**å›ç­”ï¼š**\n")
    output = re.sub(r"\$\$(.*?)\$\$", r"$$\1$$", output)
    output = re.sub(r"\\boxed\{(.*?)\}", r"\1", output)

    return output
def get_system_prompt():
    """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
    return f"""è§’è‰²è®¾å®šï¼š{st.session_state['role_config']}

åœºæ™¯è®¾å®šï¼š{st.session_state['scene_config']}

ä»»åŠ¡è¦æ±‚ï¼š{st.session_state['task_config']}

è¯·æ ¹æ®ä»¥ä¸Šè®¾å®šè¿›è¡Œå¯¹è¯ã€‚"""

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ é…ç½®è®¾ç½®")

    # Prompté…ç½®
    st.subheader("ğŸ“ Prompté…ç½®")

    # è§’è‰²é…ç½®
    st.session_state['role_config'] = st.text_area(
        "è§’è‰²é…ç½®",
        value=st.session_state['role_config'],
        height=100,
        help="å®šä¹‰AIåŠ©æ‰‹çš„è§’è‰²å’Œèº«ä»½"
    )

    # åœºæ™¯é…ç½®
    st.session_state['scene_config'] = st.text_area(
        "åœºæ™¯é…ç½®",
        value=st.session_state['scene_config'],
        height=80,
        help="è®¾å®šå¯¹è¯çš„åœºæ™¯å’Œç¯å¢ƒ"
    )

    # ä»»åŠ¡é…ç½®
    st.session_state['task_config'] = st.text_area(
        "ä»»åŠ¡é…ç½®",
        value=st.session_state['task_config'],
        height=80,
        help="æ˜ç¡®AIåŠ©æ‰‹éœ€è¦å®Œæˆçš„ä»»åŠ¡"
    )

    st.divider()

    # å‚æ•°é…ç½®
    st.subheader("ğŸ›ï¸ å‚æ•°é…ç½®")

    # Temperatureé…ç½®
    st.session_state['temperature'] = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=2.0,
        value=st.session_state['temperature'],
        step=0.1,
        help="æ§åˆ¶å›ç­”çš„åˆ›é€ æ€§ã€‚å€¼è¶Šé«˜è¶Šæœ‰åˆ›æ„ï¼Œå€¼è¶Šä½è¶Šä¿å®ˆ"
    )

    # æ˜¾ç¤ºå½“å‰é…ç½®é¢„è§ˆ
    with st.expander("ğŸ“‹ å½“å‰é…ç½®é¢„è§ˆ"):
        st.text("ç³»ç»Ÿæç¤ºè¯:")
        st.text(get_system_prompt())
        st.text(f"Temperature: {st.session_state['temperature']}")

    # é‡ç½®é…ç½®æŒ‰é’®
    if st.button("ğŸ”„ é‡ç½®ä¸ºé»˜è®¤é…ç½®"):
        st.session_state['role_config'] = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜å¹¶æä¾›å¸®åŠ©ã€‚"
        st.session_state['scene_config'] = "åœ¨ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„å¯¹è¯ç¯å¢ƒä¸­"
        st.session_state['task_config'] = "è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”"
        st.session_state['temperature'] = 0.7
        st.rerun()

    # æ¸…ç©ºå¯¹è¯å†å²æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯å†å²"):
        st.session_state["message"] = []
        st.rerun()

st.title("æ™ºè”æœªæ¥")
st.divider()  # åˆ†å‰²çº¿

# æ˜¾ç¤ºå½“å‰é…ç½®çŠ¶æ€
col1, col2 = st.columns([3, 1])
with col2:
    st.caption(f"ğŸŒ¡ï¸ Temperature: {st.session_state['temperature']}")

prompt = st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
# æ˜¯å¦ä½¿ç”¨æµå¼çš„æŒ‰é’®
use_stream = st.checkbox("ä½¿ç”¨æµå¼å“åº”", value=True)
# æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
models = ["deepseek-r1:7b", "deepseek-r1:1.5b"]  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ¨¡å‹
selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", models)


if prompt:
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state["message"].append({"role": "user", "content": prompt})

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    for message in st.session_state["message"]:
        st.chat_message(message["role"]).markdown(message["content"])

    # è·å– Ollama çš„å›å¤
    with st.spinner("æ­£åœ¨æ€è€ƒ..."):
        # è·å– Ollama çš„å›å¤
        response = client.chat(
            model='deepseek-r1:7b',
            #messages=[{"role": "user", "content": prompt}],
            messages=st.session_state["message"][-maxHistoryMessages:],
            stream=use_stream,  # æ ¹æ®æŒ‰é’®çŠ¶æ€å¯ç”¨æµå¼å“åº”
            options = {
                "temperature": st.session_state['temperature']
            }
        )

        if use_stream:
            # åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½ç¬¦
            assistant_message_placeholder = st.empty()

            # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å›å¤å†…å®¹
            assistant_message = ""

            # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            for chunk in response:
                if chunk.get("message"):
                    # è¿½åŠ æ–°çš„å†…å®¹
                    assistant_message += chunk["message"]["content"]
                    # é¢„å¤„ç†è¾“å‡º
                    assistant_message = preprocess_output(assistant_message)  
                    # é€æ­¥æ›´æ–°å ä½ç¬¦å†…å®¹
                    assistant_message_placeholder.markdown(assistant_message)
                    # æ¨¡æ‹Ÿç”Ÿæˆé€Ÿåº¦
                    time.sleep(0.05)  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´

            # æœ€ç»ˆæ·»åŠ å®Œæ•´çš„å›å¤åˆ°æ¶ˆæ¯åˆ—è¡¨
            st.session_state["message"].append({"role": "assistant", "content": assistant_message})
        else:

            response['message']['content'] = preprocess_output(response['message']['content'])  # é¢„å¤„ç†è¾“å‡º

            # æ·»åŠ ollamaçš„å›å¤
            st.session_state["message"].append({"role": "assistant", "content": response['message']['content']})
            # æ˜¾ç¤ºollamaçš„å›å¤
            st.chat_message("assistant").markdown(response['message']['content'])