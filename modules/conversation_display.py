import time
import re
import streamlit as st
import requests
from modules.tokens.API_TOKEN import get_access_token
# å¯¼å…¥MCPå·¥å…·è°ƒç”¨ç›¸å…³æ¨¡å—
from modules.mcp_client import run_async_function
local_models = ["deepseek-r1:7b", "deepseek-r1:1.5b"]
import json

def preprocess_output(output):
    # æ›¿æ¢ $$...$$ åŒ…è£¹çš„å…¬å¼ä¸º st.latex å¯è¯†åˆ«çš„å½¢å¼
    # ä¾‹å¦‚ï¼š$$\boxed{8}$$ â†’ \boxed{8}
    output = output.replace("<think>", "\n\n**æ€è€ƒï¼š**\n")
    output = output.replace("</think>", "\n\n**å›ç­”ï¼š**\n")
    output = re.sub(r"\$\$(.*?)\$\$", r"$$\1$$", output)
    output = re.sub(r"\\boxed\{(.*?)\}", r"\1", output)
    return output

def get_system_prompt():
    """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
    base_prompt = f"""è§’è‰²è®¾å®šï¼š{st.session_state['role_config']}

åœºæ™¯è®¾å®šï¼š{st.session_state['scene_config']}

ä»»åŠ¡è¦æ±‚ï¼š{st.session_state['task_config']}

è¯·æ ¹æ®ä»¥ä¸Šè®¾å®šè¿›è¡Œå¯¹è¯ã€‚"""

    # å¦‚æœå¯ç”¨äº†MCPä¸”æ˜¯è‡ªä¸»æ¨¡å¼ï¼Œæ·»åŠ å·¥å…·ä¿¡æ¯
    if (st.session_state.get("enable_mcp", False) and 
        st.session_state.get("mcp_tool_caller") and 
        st.session_state.get("auto_tool_mode", True)):
        tools_description = st.session_state["mcp_tool_caller"].get_tools_description_for_ai()
        if tools_description:
            base_prompt += "\n\n" + tools_description

    return base_prompt

def handle_ai_response_with_tools(response_text, client, selected_model, use_stream, messages):
    """å¤„ç†AIå›å¤ä¸­çš„å·¥å…·è°ƒç”¨ï¼Œå¦‚æœåŒ…å«å·¥å…·è°ƒç”¨åˆ™æ‰§è¡Œå¹¶é‡æ–°ç”Ÿæˆå›å¤"""
    # åªåœ¨è‡ªä¸»æ¨¡å¼ä¸‹å¤„ç†AIå›å¤ä¸­çš„å·¥å…·è°ƒç”¨
    if (not st.session_state.get("enable_mcp", False) or 
        not st.session_state.get("mcp_tool_caller") or
        not st.session_state.get("auto_tool_mode", True)):
        return response_text, []
    
    # æ£€æŸ¥å›å¤ä¸­æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
    mcp_results = run_async_function(
        st.session_state["mcp_tool_caller"].parse_and_execute_tools(response_text)
    )
    
    if not mcp_results:
        return response_text, []
    
    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå°†ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²ä¸­å¹¶é‡æ–°ç”Ÿæˆå›å¤
    formatted_results = st.session_state["mcp_tool_caller"].format_tool_result(mcp_results)
    
    # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
    tool_result_message = {
        "role": "system", 
        "content": f"å·¥å…·æ‰§è¡Œç»“æœï¼š\n{formatted_results}\n\nè¯·åŸºäºè¿™äº›å·¥å…·ç»“æœé‡æ–°ç”Ÿæˆå›ç­”ï¼Œä¸è¦å†æ¬¡è°ƒç”¨å·¥å…·ã€‚"
    }
    
    # é‡æ–°è°ƒç”¨æ¨¡å‹
    new_messages = messages + [{"role": "assistant", "content": response_text}, tool_result_message]
    
    try:
        if selected_model in local_models:
            new_response = client.chat(
                model=selected_model,
                messages=new_messages,
                stream=False,  # è¿™é‡Œä¸ä½¿ç”¨æµå¼ï¼Œé¿å…å¤æ‚æ€§
                options={"temperature": st.session_state['temperature']}
            )
            final_response = new_response['message']['content']
        else:
            # å¤„ç†APIæ¨¡å‹çš„æƒ…å†µ
            final_response = "åŸºäºå·¥å…·æ‰§è¡Œç»“æœï¼Œæˆ‘ç°åœ¨å¯ä»¥ä¸ºæ‚¨æä¾›æ›´å‡†ç¡®çš„ä¿¡æ¯ã€‚"
        
        return final_response, mcp_results
        
    except Exception as e:
        st.error(f"é‡æ–°ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")
        return response_text, mcp_results

def display_conversation(prompt, file_content, client, selected_model, use_stream, maxHistoryMessages, conversation_id=1, user_id=1):
    original_prompt = prompt
    if file_content is not None:
        prompt = file_content + prompt
    full_prompt = f"{file_content}\n\nç”¨æˆ·æé—®ï¼š{prompt}"

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼ˆä»…æ˜¾ç¤ºæé—®éƒ¨åˆ†ï¼‰
    user_message = {"role": "user", "content": prompt}
    st.session_state["message"].append(user_message)

    # æ˜¾ç¤ºå…ˆå‰æ¶ˆæ¯
    for message in st.session_state["message"]:
        content = message["content"]
        if message["role"] == "user" and "ç”¨æˆ·æé—®ï¼š" in content:
            content = content.split("ç”¨æˆ·æé—®ï¼š")[-1]
        st.chat_message(message["role"]).markdown(content)

    # æ£€æŸ¥æ‰‹åŠ¨å·¥å…·è°ƒç”¨æ¨¡å¼
    manual_mcp_results = []
    if (st.session_state.get("enable_mcp", False) and 
        st.session_state.get("mcp_tool_caller") and 
        not st.session_state.get("auto_tool_mode", True)):
        try:
            # å…ˆè§£æå·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_calls = st.session_state["mcp_tool_caller"]._parse_tool_calls_from_text(prompt)
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå…ˆæ˜¾ç¤ºè°ƒç”¨çš„å·¥å…·ä¿¡æ¯
            if tool_calls:
                with st.expander("ğŸ” æ£€æµ‹åˆ°çš„å·¥å…·è°ƒç”¨", expanded=True):
                    tool_info = []
                    for i, tool_call in enumerate(tool_calls, 1):
                        tool_info.append(f"**å·¥å…· {i}:**\n")
                        tool_info.append(f"- æœåŠ¡å™¨: `{tool_call['server']}`\n")
                        tool_info.append(f"- å·¥å…·å: `{tool_call['tool']}`\n")
                        tool_info.append(f"- å‚æ•°: `{json.dumps(tool_call['arguments'], ensure_ascii=False)}`\n")
                    st.markdown("".join(tool_info))
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            manual_mcp_results = run_async_function(
                st.session_state["mcp_tool_caller"].parse_and_execute_tools(prompt)
            )
            
            # å¦‚æœæœ‰æ‰‹åŠ¨å·¥å…·è°ƒç”¨ç»“æœï¼Œæ˜¾ç¤º
            if manual_mcp_results:
                with st.expander("ğŸ› ï¸ æ‰‹åŠ¨å·¥å…·æ‰§è¡Œç»“æœ", expanded=True):
                    formatted_results = st.session_state["mcp_tool_caller"].format_tool_result(manual_mcp_results)
                    st.markdown(formatted_results)
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°promptä¸­ï¼Œä¾›AIå‚è€ƒ
                tool_results_text = f"\n\nå·¥å…·æ‰§è¡Œç»“æœï¼š\n{formatted_results}"
                prompt += tool_results_text
                user_message["content"] = original_prompt  # ä¿æŒåŸå§‹ç”¨æˆ·æ¶ˆæ¯ä¸å˜
                
        except Exception as e:
            st.error(f"æ‰‹åŠ¨å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}")

    with st.spinner("æ­£åœ¨æ€è€ƒ..."):
        system_message = {"role": "system", "content": get_system_prompt()}
        user_messages = st.session_state["message"][-maxHistoryMessages:]
        messages = [system_message] + user_messages

        # è·å–APIæ¨¡å‹å›ç­”
        if selected_model not in local_models:
            payload = json.dumps({
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    },
                ],
                "temperature": st.session_state['temperature'],
                "top_p": 0.7,
                "penalty_score": 1
            })
            headers = {'Content-Type': 'application/json'}
            if selected_model == "ernie-speed-128k(æ— æµå¼API)":
                url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/ernie-speed-128k?access_token=" + get_access_token()
                response = requests.request("POST", url, headers=headers, data=payload).json()
                response = {"message": {"content": response['result']}}
        # è·å– æœ¬åœ°(Ollama) çš„å›å¤
        else:
            response = client.chat(
                model=selected_model,
                messages=messages,
                stream=use_stream,
                options={"temperature": st.session_state['temperature']}
            )
    with st.spinner("æ­£åœ¨å›ç­”..."):
        if use_stream and selected_model != "ernie-speed-128k(æ— æµå¼API)":
            assistant_message = ""
            assistant_message_placeholder = st.empty()
            for chunk in response:
                if chunk.get("message"):
                    assistant_message += chunk["message"]["content"]
                    assistant_message = preprocess_output(assistant_message)
                    assistant_message_placeholder.markdown(assistant_message)
                    time.sleep(0.05)
            
            # å¤„ç†å¯èƒ½çš„å·¥å…·è°ƒç”¨
            final_response, mcp_results = handle_ai_response_with_tools(
                assistant_message, client, selected_model, use_stream, messages
            )
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ˜¾ç¤ºå·¥å…·æ‰§è¡Œç»“æœ
            if mcp_results:
                with st.expander("ğŸ› ï¸ AIè‡ªä¸»è°ƒç”¨çš„å·¥å…·æ‰§è¡Œç»“æœ", expanded=True):
                    formatted_results = st.session_state["mcp_tool_caller"].format_tool_result(mcp_results)
                    st.markdown(formatted_results)
                
                # æ›´æ–°æ˜¾ç¤ºçš„å›å¤
                assistant_message_placeholder.markdown(preprocess_output(final_response))
                assistant_message = final_response
            
            assistant_message = {"role": "assistant", "content": assistant_message}
            st.session_state["message"].append(assistant_message)
        else:
            response['message']['content'] = preprocess_output(response['message']['content'])
            
            # å¤„ç†å¯èƒ½çš„å·¥å…·è°ƒç”¨
            final_response, mcp_results = handle_ai_response_with_tools(
                response['message']['content'], client, selected_model, use_stream, messages
            )
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ˜¾ç¤ºå·¥å…·æ‰§è¡Œç»“æœ
            if mcp_results:
                with st.expander("ğŸ› ï¸ AIè‡ªä¸»è°ƒç”¨çš„å·¥å…·æ‰§è¡Œç»“æœ", expanded=True):
                    formatted_results = st.session_state["mcp_tool_caller"].format_tool_result(mcp_results)
                    st.markdown(formatted_results)
                
                response['message']['content'] = final_response
            
            assistant_message = {"role": "assistant", "content": response['message']['content']}
            st.session_state["message"].append(assistant_message)
            st.chat_message("assistant").markdown(response['message']['content'])

    # è°ƒç”¨ Flask åç«¯æ¥å£ä¿å­˜å¯¹è¯
    save_conversation([user_message, assistant_message],  user_id, conversation_id)
    return conversation_id

def save_conversation(messages,  user_id=1, conversation_id=1):

    # æ„é€ è¯·æ±‚æ•°æ®
    data = {
        "user_id": user_id,
        "conversation_id": conversation_id,
        "messages": messages
    }

    # è°ƒç”¨ Flask åç«¯æ¥å£ä¿å­˜å¯¹è¯
    try:
        response = requests.post(f"http://127.0.0.1:5000/save_conversation", json=data)
        if response.status_code == 200:
            return conversation_id
        else:
            st.error("ä¿å­˜å¯¹è¯å¤±è´¥ã€‚")
            return None
    except Exception as e:
        st.error(f"ä¿å­˜å¯¹è¯æ—¶å‡ºé”™: {e}")
        return None

