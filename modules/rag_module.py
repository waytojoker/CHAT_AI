import json
import os
import re
import jieba
from collections import defaultdict, Counter
import hashlib
from datetime import datetime
import streamlit as st
from file_processing import read_file


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†ç±»ï¼šè´Ÿè´£æ–‡æ¡£åˆ†å—ã€å…³é”®è¯æå–å’Œç´¢å¼•æ„å»º"""

    def __init__(self, chunk_size=500, overlap=50):
        self.chunk_size = chunk_size  # æ–‡æ¡£å—å¤§å°
        self.overlap = overlap  # é‡å å­—ç¬¦æ•°
        self.stop_words = self._load_stop_words()

    def _load_stop_words(self):
        """åŠ è½½åœç”¨è¯åˆ—è¡¨"""
        # å¸¸è§ä¸­æ–‡åœç”¨è¯
        stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'å¦‚æœ', 'å¯ä»¥', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥',
            'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»–ä»¬', 'æˆ‘ä»¬', 'å®ƒä»¬', 'è¿™äº›', 'é‚£äº›', 'å·²ç»', 'è¿˜æ˜¯', 'åªæ˜¯', 'åº”è¯¥', 'å¯èƒ½', 'æˆ–è€…',
            'è™½ç„¶', 'ç„¶å', 'ä¸è¿‡', 'è€Œä¸”', 'å› æ­¤', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ä»€ä¹ˆæ—¶å€™', 'æ€ä¹ˆæ ·', 'æ¯”å¦‚', 'ä¾‹å¦‚',
            'é¦–å…ˆ', 'å…¶æ¬¡', 'æœ€å', 'å¦å¤–', 'æ­¤å¤–', 'æ€»ä¹‹', 'æ€»çš„æ¥è¯´'
        }
        return stop_words

    def split_document(self, content, filename):
        """å°†æ–‡æ¡£åˆ†å‰²æˆå—"""
        if not content or len(content.strip()) == 0:
            return []

        chunks = []
        start = 0
        chunk_id = 0

        while start < len(content):
            end = min(start + self.chunk_size, len(content))

            # å°è¯•åœ¨å¥å·ã€æ„Ÿå¹å·ã€é—®å·å¤„æ–­å¥
            if end < len(content):
                for i in range(end, max(start + self.chunk_size // 2, end - 100), -1):
                    if content[i] in 'ã€‚ï¼ï¼Ÿ\n':
                        end = i + 1
                        break

            chunk_content = content[start:end].strip()
            if chunk_content:
                chunk = {
                    'chunk_id': f"{filename}_{chunk_id}",
                    'filename': filename,
                    'content': chunk_content,
                    'start_pos': start,
                    'end_pos': end,
                    'keywords': self.extract_keywords(chunk_content)
                }
                chunks.append(chunk)
                chunk_id += 1

            # è®¡ç®—ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(start + 1, end - self.overlap)

        return chunks

    def extract_keywords(self, text):
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text)

        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = []
        for word in words:
            word = word.strip()
            if (len(word) >= 2 and
                    word not in self.stop_words and
                    not word.isspace() and
                    not word.isdigit() and
                    re.match(r'^[a-zA-Z\u4e00-\u9fa5]+$', word)):
                keywords.append(word)

        # ç»Ÿè®¡è¯é¢‘å¹¶è¿”å›å‰20ä¸ªå…³é”®è¯
        word_count = Counter(keywords)
        return [word for word, count in word_count.most_common(20)]


class DocumentIndex:
    """æ–‡æ¡£ç´¢å¼•ç±»ï¼šè´Ÿè´£æ„å»ºå’Œç»´æŠ¤å…³é”®è¯ç´¢å¼•"""

    def __init__(self, index_file="document_index.json"):
        self.index_file = index_file
        self.keyword_index = defaultdict(list)  # å…³é”®è¯ -> æ–‡æ¡£å—åˆ—è¡¨
        self.document_chunks = {}  # æ–‡æ¡£å—ID -> æ–‡æ¡£å—å†…å®¹
        self.load_index()

    def add_document_chunks(self, chunks):
        """æ·»åŠ æ–‡æ¡£å—åˆ°ç´¢å¼•"""
        for chunk in chunks:
            chunk_id = chunk['chunk_id']
            self.document_chunks[chunk_id] = chunk

            # ä¸ºæ¯ä¸ªå…³é”®è¯å»ºç«‹ç´¢å¼•
            for keyword in chunk['keywords']:
                if chunk_id not in [item['chunk_id'] for item in self.keyword_index[keyword]]:
                    self.keyword_index[keyword].append({
                        'chunk_id': chunk_id,
                        'filename': chunk['filename'],
                        'relevance': chunk['keywords'].count(keyword)  # å…³é”®è¯åœ¨è¯¥å—ä¸­çš„é¢‘æ¬¡
                    })

    def search_by_keywords(self, query, top_k=5):
        """åŸºäºå…³é”®è¯æœç´¢ç›¸å…³æ–‡æ¡£å—"""
        # æå–æŸ¥è¯¢å…³é”®è¯
        processor = DocumentProcessor()
        query_keywords = processor.extract_keywords(query)

        if not query_keywords:
            return []

        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£å—çš„ç›¸å…³æ€§å¾—åˆ†
        chunk_scores = defaultdict(float)

        for keyword in query_keywords:
            if keyword in self.keyword_index:
                for item in self.keyword_index[keyword]:
                    chunk_id = item['chunk_id']
                    # ç®€å•çš„TF-IDFè¿‘ä¼¼ï¼šè¯é¢‘ * é€†æ–‡æ¡£é¢‘ç‡
                    tf = item['relevance']
                    idf = len(self.document_chunks) / len(self.keyword_index[keyword])
                    chunk_scores[chunk_id] += tf * idf

        # æŒ‰å¾—åˆ†æ’åºå¹¶è¿”å›top_kä¸ªç»“æœ
        sorted_chunks = sorted(chunk_scores.items(), key=lambda x: x[1], reverse=True)

        results = []
        for chunk_id, score in sorted_chunks[:top_k]:
            if chunk_id in self.document_chunks:
                chunk = self.document_chunks[chunk_id].copy()
                chunk['relevance_score'] = score
                results.append(chunk)

        return results

    def save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        index_data = {
            'keyword_index': dict(self.keyword_index),
            'document_chunks': self.document_chunks,
            'last_updated': datetime.now().isoformat()
        }

        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

    def load_index(self):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        if os.path.exists(self.index_file):
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                    self.keyword_index = defaultdict(list, index_data.get('keyword_index', {}))
                    self.document_chunks = index_data.get('document_chunks', {})
            except Exception as e:
                print(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
                self.keyword_index = defaultdict(list)
                self.document_chunks = {}

    def delete_document(self, filename):
        """åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£å—"""
        # æ‰¾åˆ°è¦åˆ é™¤çš„chunk_ids
        chunks_to_delete = [chunk_id for chunk_id, chunk in self.document_chunks.items()
                            if chunk['filename'] == filename]

        # ä»document_chunksä¸­åˆ é™¤
        for chunk_id in chunks_to_delete:
            del self.document_chunks[chunk_id]

        # ä»keyword_indexä¸­åˆ é™¤
        for keyword in list(self.keyword_index.keys()):
            self.keyword_index[keyword] = [item for item in self.keyword_index[keyword]
                                           if item['chunk_id'] not in chunks_to_delete]
            # å¦‚æœæŸä¸ªå…³é”®è¯æ²¡æœ‰å¯¹åº”çš„æ–‡æ¡£å—äº†ï¼Œåˆ é™¤è¯¥å…³é”®è¯
            if not self.keyword_index[keyword]:
                del self.keyword_index[keyword]


class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»ï¼šæ•´åˆæ–‡æ¡£å¤„ç†ã€ç´¢å¼•å’Œæ£€ç´¢åŠŸèƒ½"""

    def __init__(self):
        self.processor = DocumentProcessor()
        self.index = DocumentIndex()

    def add_document(self, file_obj, filename=None):
        """æ·»åŠ æ–‡æ¡£åˆ°RAGç³»ç»Ÿ"""
        if filename is None:
            filename = getattr(file_obj, 'name', 'unknown_file')

        # è¯»å–æ–‡ä»¶å†…å®¹
        content = read_file(file_obj)
        if not content:
            return False, "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"

        # åˆ é™¤åŒåæ–‡ä»¶çš„æ—§ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.index.delete_document(filename)

        # åˆ†å‰²æ–‡æ¡£
        chunks = self.processor.split_document(content, filename)
        if not chunks:
            return False, "æ–‡æ¡£åˆ†å‰²å¤±è´¥"

        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add_document_chunks(chunks)

        # ä¿å­˜ç´¢å¼•
        self.index.save_index()

        return True, f"æˆåŠŸå¤„ç†æ–‡æ¡£ {filename}ï¼Œåˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æ¡£å—"

    def search_documents(self, query, top_k=3):
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        return self.index.search_by_keywords(query, top_k)

    def generate_rag_prompt(self, query, context_chunks):
        """ç”ŸæˆåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºè¯"""
        if not context_chunks:
            return query

        context_text = "\n\n".join([
            f"æ–‡æ¡£ç‰‡æ®µ {i + 1} (æ¥æº: {chunk['filename']}):\n{chunk['content']}"
            for i, chunk in enumerate(context_chunks)
        ])

        rag_prompt = f"""è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚

ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"""

        return rag_prompt

    def get_document_stats(self):
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        total_chunks = len(self.index.document_chunks)
        total_keywords = len(self.index.keyword_index)

        # æŒ‰æ–‡ä»¶ç»Ÿè®¡
        file_stats = defaultdict(int)
        for chunk in self.index.document_chunks.values():
            file_stats[chunk['filename']] += 1

        return {
            'total_chunks': total_chunks,
            'total_keywords': total_keywords,
            'files': dict(file_stats)
        }

    def delete_document(self, filename):
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        self.index.delete_document(filename)
        self.index.save_index()
        return f"å·²åˆ é™¤æ–‡æ¡£: {filename}"


# Streamlitç•Œé¢ç»„ä»¶
def show_rag_management():
    """æ˜¾ç¤ºRAGæ–‡æ¡£ç®¡ç†ç•Œé¢"""
    st.subheader("ğŸ“š ç§æœ‰æ–‡æ¡£ç®¡ç†")

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = RAGSystem()

    rag_system = st.session_state.rag_system

    # æ–‡æ¡£ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£",
        type=["txt", "pdf", "docx", "xlsx", "pptx"],
        accept_multiple_files=True,
        help="æ”¯æŒçš„æ ¼å¼ï¼šTXT, PDF, Word, Excel, PowerPoint"
    )

    if uploaded_files:
        for uploaded_file in uploaded_files:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"ğŸ“„ {uploaded_file.name}")
            with col2:
                if st.button(f"å¤„ç†", key=f"process_{uploaded_file.name}"):
                    with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                        success, message = rag_system.add_document(uploaded_file, uploaded_file.name)
                        if success:
                            st.success(message)
                        else:
                            st.error(message)
                        st.rerun()

    # æ˜¾ç¤ºæ–‡æ¡£ç»Ÿè®¡
    stats = rag_system.get_document_stats()

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("æ–‡æ¡£å—æ•°é‡", stats['total_chunks'])
    with col2:
        st.metric("å…³é”®è¯æ•°é‡", stats['total_keywords'])
    with col3:
        st.metric("æ–‡æ¡£æ•°é‡", len(stats['files']))

    # æ˜¾ç¤ºå·²å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
    if stats['files']:
        st.subheader("ğŸ“‹ å·²å¤„ç†æ–‡æ¡£")
        for filename, chunk_count in stats['files'].items():
            col1, col2 = st.columns([4, 1])
            with col1:
                st.write(f"ğŸ“„ {filename} ({chunk_count} ä¸ªæ–‡æ¡£å—)")
            with col2:
                if st.button("åˆ é™¤", key=f"delete_{filename}"):
                    rag_system.delete_document(filename)
                    st.success(f"å·²åˆ é™¤: {filename}")
                    st.rerun()


def enhance_query_with_rag(query, use_rag=True):
    """ä½¿ç”¨RAGå¢å¼ºæŸ¥è¯¢"""
    if not use_rag or 'rag_system' not in st.session_state:
        return query, []

    rag_system = st.session_state.rag_system

    # æœç´¢ç›¸å…³æ–‡æ¡£
    relevant_chunks = rag_system.search_documents(query, top_k=3)

    if not relevant_chunks:
        return query, []

    # ç”Ÿæˆå¢å¼ºçš„æç¤ºè¯
    enhanced_prompt = rag_system.generate_rag_prompt(query, relevant_chunks)

    return enhanced_prompt, relevant_chunks


# æµ‹è¯•å‡½æ•°
def test_rag_system():
    """æµ‹è¯•RAGç³»ç»ŸåŠŸèƒ½"""
    rag_system = RAGSystem()

    # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹
    test_content = """
    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›é€ èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
    è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
    """

    # æ¨¡æ‹Ÿæ–‡ä»¶å¯¹è±¡
    class MockFile:
        def __init__(self, content, name):
            self.content = content
            self.name = name
            self.type = "text/plain"

        def getvalue(self):
            return self.content.encode('utf-8')

    mock_file = MockFile(test_content, "ai_knowledge.txt")

    # æ·»åŠ æ–‡æ¡£
    success, message = rag_system.add_document(mock_file)
    print(f"æ·»åŠ æ–‡æ¡£: {success}, {message}")

    # æœç´¢æµ‹è¯•
    queries = ["ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "æ·±åº¦å­¦ä¹ çš„åŸç†", "NLPæ˜¯ä»€ä¹ˆ"]

    for query in queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = rag_system.search_documents(query)
        for i, result in enumerate(results):
            print(f"ç»“æœ {i + 1} (ç›¸å…³æ€§: {result['relevance_score']:.2f}): {result['content'][:100]}...")


if __name__ == "__main__":
    test_rag_system()
